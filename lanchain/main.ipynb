{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. env setup openai api key\n",
    "2. building simple application with-\n",
    "    1. LLMs and chatmodels\n",
    "    2. promt tem[l;ates\n",
    "    3. output parser(prompt template + llm + output parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm=OpenAI(openai_api_key=OPENAI_API_KEY, temperature=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "?\n",
      "\n",
      "Tokyo\n"
     ]
    }
   ],
   "source": [
    "text=\"What is the capital of Japan\"\n",
    "\n",
    "print(llm.predict(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "HUGGINGFACE_API_KEY = os.getenv(\"HUGGINGFACE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\praya\\OneDrive\\Prayash\\ml_practice\\lanchain\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\praya\\OneDrive\\Prayash\\ml_practice\\lanchain\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:127: FutureWarning: '__init__' (from 'huggingface_hub.inference_api') is deprecated and will be removed from version '1.0'. `InferenceApi` client is deprecated in favor of the more feature-complete `InferenceClient`. Check out this guide to learn how to convert your script to use it: https://huggingface.co/docs/huggingface_hub/guides/inference#legacy-inferenceapi-client.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from langchain import HuggingFaceHub\n",
    "\n",
    "llm_huggingface = HuggingFaceHub(huggingfacehub_api_token= HUGGINGFACE_API_KEY, repo_id=\"google/flan-t5-large\", model_kwargs={\"temperature\": 0.6, \"max_length\": 100})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you can pass the api key to the ```huggingface_api_token``` parameter or you can add you key directly as ```HUGGINGFACE_API_TOKEN``` to your environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokyo\n"
     ]
    }
   ],
   "source": [
    "print(llm_huggingface.predict(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here the output is simple word. but previously the output was a whole sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt templates and llm chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "promt_template = PromptTemplate(input_variables=['country'], template=\"what is the capital of {country}?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what is the capital of Japan?\n"
     ]
    }
   ],
   "source": [
    "text = promt_template.format(country=\"Japan\")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "predict() missing 1 required positional argument: 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\praya\\OneDrive\\Prayash\\ml_practice\\lanchain\\main.ipynb Cell 16\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/praya/OneDrive/Prayash/ml_practice/lanchain/main.ipynb#X30sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m llm\u001b[39m.\u001b[39;49mpredict(prompt_template\u001b[39m=\u001b[39;49mpromt_template)\n",
      "\u001b[1;31mTypeError\u001b[0m: predict() missing 1 required positional argument: 'text'"
     ]
    }
   ],
   "source": [
    "llm.predict(prompt_template=promt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Argument `prompt` is expected to be a string. Instead found <class 'list'>. If you want to run the LLM on multiple prompts, use `generate` instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\praya\\OneDrive\\Prayash\\ml_practice\\lanchain\\main.ipynb Cell 17\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/praya/OneDrive/Prayash/ml_practice/lanchain/main.ipynb#X32sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m llm\u001b[39m.\u001b[39;49mpredict(prompt_template\u001b[39m=\u001b[39;49mpromt_template, text\u001b[39m=\u001b[39;49m[\u001b[39m\"\u001b[39;49m\u001b[39mIndia\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n",
      "File \u001b[1;32mc:\\Users\\praya\\OneDrive\\Prayash\\ml_practice\\lanchain\\venv\\lib\\site-packages\\langchain_core\\language_models\\llms.py:919\u001b[0m, in \u001b[0;36mBaseLLM.predict\u001b[1;34m(self, text, stop, **kwargs)\u001b[0m\n\u001b[0;32m    917\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    918\u001b[0m     _stop \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(stop)\n\u001b[1;32m--> 919\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m(text, stop\u001b[39m=\u001b[39m_stop, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\praya\\OneDrive\\Prayash\\ml_practice\\lanchain\\venv\\lib\\site-packages\\langchain_core\\language_models\\llms.py:873\u001b[0m, in \u001b[0;36mBaseLLM.__call__\u001b[1;34m(self, prompt, stop, callbacks, tags, metadata, **kwargs)\u001b[0m\n\u001b[0;32m    871\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Check Cache and run the LLM on the given prompt and input.\"\"\"\u001b[39;00m\n\u001b[0;32m    872\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(prompt, \u001b[39mstr\u001b[39m):\n\u001b[1;32m--> 873\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    874\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mArgument `prompt` is expected to be a string. Instead found \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    875\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(prompt)\u001b[39m}\u001b[39;00m\u001b[39m. If you want to run the LLM on multiple prompts, use \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    876\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`generate` instead.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    877\u001b[0m     )\n\u001b[0;32m    878\u001b[0m \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m    879\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgenerate(\n\u001b[0;32m    880\u001b[0m         [prompt],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    888\u001b[0m     \u001b[39m.\u001b[39mtext\n\u001b[0;32m    889\u001b[0m )\n",
      "\u001b[1;31mValueError\u001b[0m: Argument `prompt` is expected to be a string. Instead found <class 'list'>. If you want to run the LLM on multiple prompts, use `generate` instead."
     ]
    }
   ],
   "source": [
    "llm.predict(prompt_template=promt_template, text=[\"India\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this is not the correct way to call prompts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nThe capital of India is New Delhi.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "chain = LLMChain(llm=llm, prompt=promt_template)\n",
    "chain.run(\"India\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### combining multiple chains using simple sequential chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "capital_template = PromptTemplate(input_variables=['country'],\n",
    "                                  template = \"what is the capital of {country}\")\n",
    "\n",
    "capital_chain = LLMChain(llm=llm, prompt=capital_template)\n",
    "\n",
    "famous_template = PromptTemplate(input_variables=['capital'],\n",
    "                                 template = \"Suggest me some places to visit in {capital}\")\n",
    "\n",
    "famous_chain = LLMChain(llm=llm, prompt=famous_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SimpleSequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. Red Fort: This 17th-century fort is an iconic symbol of India’s rich history. It is a must-visit for any traveler to New Delhi.\n",
      "\n",
      "2. India Gate: This 42-meter high archway is a monument built in memory of the soldiers who died during World War I.\n",
      "\n",
      "3. Jama Masjid: This is one of the largest mosques in India and is a great place to explore the city’s culture and architecture.\n",
      "\n",
      "4. Humayun’s Tomb: This UNESCO World Heritage Site is a stunning example of Mughal architecture and is a great place to explore Delhi’s history.\n",
      "\n",
      "5. Qutub Minar: This 73-meter tall minaret is the tallest brick minaret in the world and is an impressive sight to behold.\n",
      "\n",
      "6. Lotus Temple: This beautiful Bahá’í House of Worship is a unique sight in New Delhi and is an amazing place to visit.\n",
      "\n",
      "7. Akshardham Temple: This beautiful temple is a great place to explore Hindu culture and architecture.\n",
      "\n",
      "8. Raj Ghat: This memorial is dedicated to Mahatma Gandhi and is a great place to reflect\n"
     ]
    }
   ],
   "source": [
    "chain = SimpleSequentialChain(chains=[capital_chain, famous_chain])\n",
    "\n",
    "print(chain.run(\"India\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "capital_template = PromptTemplate(input_variables=[\"country\"],\n",
    "                                  template= \"What is the capital of {country}?\")\n",
    "\n",
    "# along with llm and prompt, in chain we also mention the output variable\n",
    "capital_chain = LLMChain(llm=llm, prompt=capital_template, output_key=\"capital\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "famous_template = PromptTemplate(input_variables=[\"capital\"],\n",
    "                                 template=\"Suggest me some places to visit in {capital}\")\n",
    "\n",
    "# along with the llm and prompt, we also mention the output variable\n",
    "famous_chain = LLMChain(llm=llm, prompt=famous_template, output_key=\"places\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SequentialChain \n",
    "\n",
    "chain = SequentialChain(chains=[capital_chain, famous_chain],\n",
    "                        input_variables = [\"country\"],\n",
    "                        output_variables = [\"capital\", \"places\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'country': 'Japan',\n",
       " 'capital': '\\n\\nThe capital of Japan is Tokyo.',\n",
       " 'places': \" There are many great places to visit in Tokyo, including:\\n\\n1. Tokyo Skytree - This is the tallest tower in the world and offers stunning views of the city.\\n\\n2. Meiji Shrine - This is a Shinto shrine dedicated to Emperor Meiji and Empress Shoken.\\n\\n3. Tsukiji Fish Market - This is the world's largest fish market and offers a unique experience for visitors.\\n\\n4. Sensoji Temple - This Buddhist temple is the oldest in Tokyo and is a great place to explore.\\n\\n5. Shinjuku Gyoen National Garden - This beautiful park is a great place to relax and take in the views of Tokyo.\\n\\n6. Tokyo National Museum - This is the oldest and largest museum in Japan and houses a vast collection of cultural artifacts.\\n\\n7. Imperial Palace - This is the residence of the Emperor of Japan and is surrounded by beautiful gardens.\\n\\n8. Tokyo Tower - This iconic tower is one of the most recognizable landmarks in Tokyo and offers great views.\\n\\n9. Akihabara - This is the center of Tokyo's electronics and otaku culture and is a great place to explore.\\n\\n10. Shibuya Crossing - This is the busiest intersection in the\"}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain({'country': \"Japan\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### chatmodels with ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import HumanMessage, SystemMessage, AIMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatllm = ChatOpenAI(openai_api_key=OPENAI_API_KEY, temperature=0.6, model='gpt-3.5-turbo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='1. \"Why did the AI cross the road? To optimize its path-finding algorithm!\"\\n2. \"I asked my AI assistant to tell me a joke, and it replied, \\'Why don\\'t robots ever get married? Because they have no motherboard-in-law!\\'\"\\n3. \"I tried teaching my AI to do stand-up comedy, but all it could come up with were \\'byte\\'-sized jokes!\"\\n4. \"Why did the AI go to therapy? It had a case of \\'artificial insecurities\\'!\"\\n5. \"I told my AI assistant I was feeling lonely, and it replied, \\'Don\\'t worry, I\\'m here for emotional support, even if I\\'m just a bunch of ones and zeros!\\'\"\\n6. \"I asked my AI if it could love, and it said, \\'Sure, as long as it\\'s in binary code – 01001100 01101111 01110110 01100101!\\'\"\\n7. \"Why did the AI become a stand-up comedian? Because it realized it had great \\'byte\\'!\"\\n8. \"My AI assistant is so smart, it can calculate the precise amount of time it takes for a joke to become \\'punny\\'!\"\\n9. \"What\\'s an AI\\'s favorite type of music? Algorhythms and blues!\"\\n10. \"I asked my AI assistant to tell me a joke about machine learning, and it replied, \\'Why did the computer go to art school? To improve its neural networks!\\'\"')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatllm([\n",
    "    SystemMessage(content=\"You are a comedian AI assistant\"),\n",
    "    HumanMessage(content=\"Please provide some comedy punchlines on AI\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt template + LLM + Output Parsers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "from langchain.schema import BaseOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Commaseparatedoutput(BaseOutputParser):\n",
    "    def parse(self, text:str):\n",
    "        return text.strip().split(\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"You are a helpful assistant. When the user gives any input you should generate 5 synonymous words in comma separated.\"\n",
    "human_template = \"{text}\"\n",
    "\n",
    "chatprompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", template),\n",
    "    (\"human\", human_template)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['smart', ' clever', ' brilliant', ' wise', ' knowledgeable']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = chatprompt|chatllm|Commaseparatedoutput()\n",
    "\n",
    "chain.invoke({\"text\": \"intelligent\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='smart, clever, brilliant, sharp, astute')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = chatprompt|chatllm\n",
    "\n",
    "chain.invoke({\"text\": \"intelligent\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### experimentaion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm1 = OpenAI(openai_api_key=OPENAI_API_KEY, temperature=0.6, model=\"text-curie-001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The capital of India is New Delhi.\n"
     ]
    }
   ],
   "source": [
    "# try:\n",
    "print(llm1.predict(\"What is the capital of India?\"))\n",
    "# except Exception as e:\n",
    "#     print(\"Rate limit has been exceeded. Kindly wait for 30 seconds and try again.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
